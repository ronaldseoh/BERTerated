{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6qruq9ZFQsN"
   },
   "source": [
    "# Further Pre-training MobileBERT MLM with Centralized Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-AgOn_qCFQsk"
   },
   "outputs": [],
   "source": [
    "# Copyright 2020, The TensorFlow Federated Authors.\n",
    "# Copyright 2020, Ronald Seoh\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zUGmvEbFQts"
   },
   "source": [
    "## Google Colab settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26721,
     "status": "ok",
     "timestamp": 1605903791554,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "_1nBG9qaFQt9",
    "outputId": "d92c0e3d-138a-4973-b7f5-7e4e8351febb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/My Drive/Colab Notebooks/BERTerated\n",
      "datasets\n",
      "fedavg_client.py\n",
      "fedavg.py\n",
      "huggingface_keras_layers.py\n",
      "LICENSE\n",
      "mobilebert_mlm_shakespeare_centralized.ipynb\n",
      "mobilebert_mlm_shakespeare_fedavg.ipynb\n",
      "mobilebert_mlm_stackoverflow_fedavg.ipynb\n",
      "__pycache__\n",
      "README.md\n",
      "requirements.txt\n",
      "tff_cache\n",
      "transformers_cache\n",
      "utils.py\n"
     ]
    }
   ],
   "source": [
    "# Use Google Colab\n",
    "use_colab = True\n",
    "\n",
    "# Is this notebook running on Colab?\n",
    "# If so, then google.colab package (github.com/googlecolab/colabtools)\n",
    "# should be available in this environment\n",
    "\n",
    "# Previous version used importlib, but we could do the same thing with\n",
    "# just attempting to import google.colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    colab_available = True\n",
    "except:\n",
    "    colab_available = False\n",
    "\n",
    "if use_colab and colab_available:\n",
    "    # Mount Google Drive root directory\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # cd to the appropriate working directory under my Google Drive\n",
    "    %cd '/content/drive/My Drive/Colab Notebooks/BERTerated'\n",
    "    \n",
    "    # List the directory contents\n",
    "    !ls\n",
    "\n",
    "# IPython reloading magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45950,
     "status": "ok",
     "timestamp": 1605903810801,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "seTmBYc5EJ7q",
    "outputId": "65cb57f9-5104-4d55-ff6a-6b03748d77f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-federated==0.17.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/54/900d99d3cff21b6a570281b51f4878a745c0eece7732bb7fc26eee61ef57/tensorflow_federated-0.17.0-py2.py3-none-any.whl (517kB)\n",
      "\r",
      "\u001b[K     |▋                               | 10kB 25.6MB/s eta 0:00:01\r",
      "\u001b[K     |█▎                              | 20kB 33.2MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 30kB 38.6MB/s eta 0:00:01\r",
      "\u001b[K     |██▌                             | 40kB 33.4MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 51kB 35.1MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 61kB 37.9MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 71kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 81kB 22.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 92kB 23.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████▍                         | 102kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 112kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 122kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 133kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████▉                       | 143kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 153kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 163kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▊                     | 174kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 184kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 194kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▊                   | 204kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 215kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 225kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 235kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 245kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 256kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 266kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 276kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▊              | 286kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 296kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 307kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 317kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 327kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 337kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▌          | 348kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 358kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▉         | 368kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 378kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 389kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 399kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 409kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 419kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▋     | 430kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 440kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 450kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 460kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 471kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 481kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 491kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 501kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▊| 512kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 522kB 22.7MB/s \n",
      "\u001b[?25hCollecting tensorflow-text==2.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/b2/2dbd90b93913afd07e6101b8b84327c401c394e60141c1e98590038060b3/tensorflow_text-2.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.6MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6MB 20.3MB/s \n",
      "\u001b[?25hCollecting transformers==3.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 31.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: portpicker~=1.3.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (1.3.1)\n",
      "Collecting attrs~=19.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/db/4313ab3be961f7a763066401fb77f7748373b6094076ae2bda2806988af6/attrs-19.3.0-py2.py3-none-any.whl\n",
      "Collecting cachetools~=3.1.1\n",
      "  Downloading https://files.pythonhosted.org/packages/2f/a6/30b0a0bef12283e83e58c1d6e7b5aabc7acfc4110df81a4471655d33e704/cachetools-3.1.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (0.1.5)\n",
      "Collecting tensorflow-model-optimization~=0.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/cc/4b0831f492396f03a4563cc749ad94cbf7af986aaa7a7d89e5979029ce5c/tensorflow_model_optimization-0.4.1-py2.py3-none-any.whl (172kB)\n",
      "\u001b[K     |████████████████████████████████| 174kB 58.6MB/s \n",
      "\u001b[?25hCollecting grpcio~=1.29.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/04/2b67f0a3645481235d5547891fd0e45e384f1ae5676788f24a7c8735b4e9/grpcio-1.29.0-cp36-cp36m-manylinux2010_x86_64.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 49.6MB/s \n",
      "\u001b[?25hCollecting tensorflow-addons~=0.11.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/f8/d6fca180c123f2851035c4493690662ebdad0849a9059d56035434bff5c9/tensorflow_addons-0.11.2-cp36-cp36m-manylinux2010_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 35.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (2.10.0)\n",
      "Collecting tensorflow-privacy~=0.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/ae/7db0dcf76a746314a174578a7b99ff098b40b908c4c693a955a2bbc0127b/tensorflow_privacy-0.5.1-py3-none-any.whl (149kB)\n",
      "\u001b[K     |████████████████████████████████| 153kB 57.6MB/s \n",
      "\u001b[?25hCollecting absl-py~=0.9.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/53/9243c600e047bd4c3df9e69cfabc1e8004a82cac2e0c484580a78a94ba2a/absl-py-0.9.0.tar.gz (104kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 58.4MB/s \n",
      "\u001b[?25hCollecting semantic-version~=2.8.5\n",
      "  Downloading https://files.pythonhosted.org/packages/a5/15/00ef3b7888a10363b7c402350eda3acf395ff05bebae312d1296e528516a/semantic_version-2.8.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy~=1.18.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (1.18.5)\n",
      "Requirement already satisfied: tensorflow~=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: retrying~=1.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (1.3.3)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 45.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0->-r requirements.txt (line 3)) (4.41.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0->-r requirements.txt (line 3)) (2019.12.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0->-r requirements.txt (line 3)) (2.23.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0->-r requirements.txt (line 3)) (0.7)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0->-r requirements.txt (line 3)) (3.12.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0->-r requirements.txt (line 3)) (20.4)\n",
      "Collecting tokenizers==0.9.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 48.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.4.0->-r requirements.txt (line 3)) (3.0.12)\n",
      "Collecting sentencepiece!=0.1.92\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 44.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from dm-tree~=0.1.1->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons~=0.11.1->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (2.7.1)\n",
      "Requirement already satisfied: tensorflow-estimator>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-privacy~=0.5.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from tensorflow-privacy~=0.5.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (1.4.1)\n",
      "Requirement already satisfied: mpmath in /usr/local/lib/python3.6/dist-packages (from tensorflow-privacy~=0.5.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (0.35.1)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (0.3.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (1.12.1)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.4.0->-r requirements.txt (line 3)) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.4.0->-r requirements.txt (line 3)) (0.17.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.4.0->-r requirements.txt (line 3)) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.4.0->-r requirements.txt (line 3)) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.4.0->-r requirements.txt (line 3)) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.4.0->-r requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.4.0->-r requirements.txt (line 3)) (50.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.4.0->-r requirements.txt (line 3)) (2.4.7)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (3.3.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (1.7.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (1.17.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (0.4.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (3.4.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->tensorflow-federated==0.17.0->-r requirements.txt (line 1)) (3.1.0)\n",
      "Building wheels for collected packages: absl-py, sacremoses\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for absl-py: filename=absl_py-0.9.0-cp36-none-any.whl size=121933 sha256=b5cd32b587e0abbc6a39ff9d449352cd1f365781c4819e49e09e894215ca3425\n",
      "  Stored in directory: /root/.cache/pip/wheels/8e/28/49/fad4e7f0b9a1227708cbbee4487ac8558a7334849cb81c813d\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=b84a920de19a96e49b88c2bce6b12050eaeecc0306ee46e2e51c50321c131757\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built absl-py sacremoses\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: attrs, cachetools, tensorflow-model-optimization, grpcio, tensorflow-addons, tensorflow-privacy, absl-py, semantic-version, tensorflow-federated, tensorflow-text, sacremoses, tokenizers, sentencepiece, transformers\n",
      "  Found existing installation: attrs 20.2.0\n",
      "    Uninstalling attrs-20.2.0:\n",
      "      Successfully uninstalled attrs-20.2.0\n",
      "  Found existing installation: cachetools 4.1.1\n",
      "    Uninstalling cachetools-4.1.1:\n",
      "      Successfully uninstalled cachetools-4.1.1\n",
      "  Found existing installation: grpcio 1.33.2\n",
      "    Uninstalling grpcio-1.33.2:\n",
      "      Successfully uninstalled grpcio-1.33.2\n",
      "  Found existing installation: tensorflow-addons 0.8.3\n",
      "    Uninstalling tensorflow-addons-0.8.3:\n",
      "      Successfully uninstalled tensorflow-addons-0.8.3\n",
      "  Found existing installation: tensorflow-privacy 0.2.2\n",
      "    Uninstalling tensorflow-privacy-0.2.2:\n",
      "      Successfully uninstalled tensorflow-privacy-0.2.2\n",
      "  Found existing installation: absl-py 0.10.0\n",
      "    Uninstalling absl-py-0.10.0:\n",
      "      Successfully uninstalled absl-py-0.10.0\n",
      "Successfully installed absl-py-0.9.0 attrs-19.3.0 cachetools-3.1.1 grpcio-1.29.0 sacremoses-0.0.43 semantic-version-2.8.5 sentencepiece-0.1.94 tensorflow-addons-0.11.2 tensorflow-federated-0.17.0 tensorflow-model-optimization-0.4.1 tensorflow-privacy-0.5.1 tensorflow-text-2.3.0 tokenizers-0.9.2 transformers-3.4.0\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F16eB2w-FQuw"
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58490,
     "status": "ok",
     "timestamp": 1605903823358,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "k_SnruV2FQu0",
    "outputId": "f5a2da82-fcfe-412c-fbc9-3b4ad4e01e0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'Hello, World!'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import tensorflow_text as tf_text\n",
    "import transformers\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import fedavg\n",
    "import fedavg_client\n",
    "import datasets\n",
    "import utils\n",
    "\n",
    "# Random seed settings\n",
    "random_seed = 692\n",
    "random.seed(random_seed) # Python\n",
    "np.random.seed(random_seed) # NumPy\n",
    "tf.random.set_seed(random_seed) # TensorFlow\n",
    "\n",
    "# Tensorflow GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Test the TFF is working:\n",
    "tff.federated_computation(lambda: 'Hello, World!')()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58476,
     "status": "ok",
     "timestamp": 1605903823362,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "FHYxELiQFQvI",
    "outputId": "cbc5cd58-bfe8-4c18-bb43-6c89a9a56bde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.6.9 (default, Oct  8 2020, 12:12:24) \n",
      "[GCC 8.4.0]\n",
      "NumPy version: 1.18.5\n",
      "TensorFlow version: 2.3.0\n",
      "TensorFlow Federated version: 0.17.0\n",
      "Transformers version: 3.4.0\n"
     ]
    }
   ],
   "source": [
    "# Print version information\n",
    "print(\"Python version: \" + sys.version)\n",
    "print(\"NumPy version: \" + np.__version__)\n",
    "print(\"TensorFlow version: \" + tf.__version__)\n",
    "print(\"TensorFlow Federated version: \" + tff.__version__)\n",
    "print(\"Transformers version: \" + transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYXMEboAFQvd"
   },
   "source": [
    "## Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M071lqOQFQvi"
   },
   "outputs": [],
   "source": [
    "TOTAL_ROUNDS = 2 # Number of total training rounds\n",
    "ROUNDS_PER_EVAL = 1 # How often to evaluate\n",
    "\n",
    "# Number of epochs in the client to take per round.\n",
    "CLIENT_EPOCHS_PER_ROUND = 3\n",
    "\n",
    "# Epochs to train in centralized setting\n",
    "CENTRALIZED_EPOCHS = CLIENT_EPOCHS_PER_ROUND * TOTAL_ROUNDS\n",
    "\n",
    "BATCH_SIZE = 8 # Batch size used on the client.\n",
    "TEST_BATCH_SIZE = 8 # Minibatch size of test data.\n",
    "\n",
    "# Maximum length of input token sequence for BERT.\n",
    "BERT_MAX_SEQ_LENGTH = 128\n",
    "\n",
    "# Optimizer configuration\n",
    "LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-tHcHX0FQvz"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuI6ktSIbCjk"
   },
   "source": [
    "### Load the Shakespeare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVkK1BV9FQv3"
   },
   "outputs": [],
   "source": [
    "train_client_data, test_client_data = tff.simulation.datasets.shakespeare.load_data(cache_dir='./tff_cache')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNYlFQJ3Wt1O"
   },
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXmJYQbeFQwG"
   },
   "outputs": [],
   "source": [
    "mobilebert_tokenizer = transformers.MobileBertTokenizer.from_pretrained(\n",
    "    'google/mobilebert-uncased', cache_dir='./transformers_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8c7S9s9qRlsi"
   },
   "outputs": [],
   "source": [
    "# Imitate transformers tokenizer with TF.Text Tokenizer\n",
    "tokenizer_tf_text, vocab_lookup_table, special_ids_mask_table = datasets.preprocessing_for_bert.convert_huggingface_tokenizer(mobilebert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "executionInfo": {
     "elapsed": 61197,
     "status": "ok",
     "timestamp": 1605903826126,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "5iwW2jo3TFUt",
    "outputId": "060f7fca-d799-4bd5-8faf-c15b219861df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Text tokenizer output shape: tf.Tensor([1 5 1], shape=(3,), dtype=int32)\n",
      "tf.Tensor([[2023 2003 1037 3231 1012]], shape=(1, 5), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'this is a test.'"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test if our new tokenizer works\n",
    "ttt = tokenizer_tf_text.tokenize(\"This is a test.\")\n",
    "print(\"TF Text tokenizer output shape:\", tf.shape(ttt.to_tensor()))\n",
    "print(tf.squeeze(ttt.to_tensor(), axis=-1))\n",
    "mobilebert_tokenizer.decode(tf.squeeze(ttt, axis=-1).to_list()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sfMNvpmFQwW"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oIOw1zG1RfnU"
   },
   "outputs": [],
   "source": [
    "def check_empty_snippet(x):\n",
    "    return tf.strings.length(x['snippets']) > 0\n",
    "\n",
    "def tokenizer_and_mask_wrapped(x):\n",
    "\n",
    "    masked, labels, sample_weights = datasets.preprocessing_for_bert.tokenize_and_mask(tf.reshape(x['snippets'], shape=[1]),\n",
    "                                                                                       max_seq_length=BERT_MAX_SEQ_LENGTH,\n",
    "                                                                                       bert_tokenizer_tf_text=tokenizer_tf_text,\n",
    "                                                                                       vocab_lookup_table=vocab_lookup_table,\n",
    "                                                                                       special_ids_mask_table=special_ids_mask_table,\n",
    "                                                                                       cls_token_id=mobilebert_tokenizer.cls_token_id,\n",
    "                                                                                       sep_token_id=mobilebert_tokenizer.sep_token_id,\n",
    "                                                                                       pad_token_id=mobilebert_tokenizer.pad_token_id,\n",
    "                                                                                       mask_token_id=mobilebert_tokenizer.mask_token_id)\n",
    "\n",
    "    return (masked, labels, sample_weights)\n",
    "\n",
    "def preprocess_for_train(train_dataset):\n",
    "    return (\n",
    "        # Filter out empty strings\n",
    "        train_dataset#.filter(lambda x: tf.strings.length(x['snippets']) > 0)\n",
    "        # Tokenize each samples using MobileBERT tokenizer\n",
    "        .map(tokenizer_and_mask_wrapped, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        # Shuffle\n",
    "        .shuffle(100000)\n",
    "        # Form minibatches\n",
    "        # Use drop_remainder=True to force the batch size to be exactly BATCH_SIZE\n",
    "        # and make the shape **exactly** (BATCH_SIZE, SEQ_LENGTH)\n",
    "        #.batch(BATCH_SIZE, drop_remainder=True)\n",
    "        # Repeat to make each client train multiple epochs\n",
    "        .repeat(count=CENTRALIZED_EPOCHS))\n",
    "    \n",
    "def preprocess_for_test(test_dataset):\n",
    "    return (\n",
    "        # Filter out empty strings\n",
    "        test_dataset#.filter(lambda x: tf.strings.length(x['snippets']) > 0)\n",
    "        # Tokenize each samples using MobileBERT tokenizer\n",
    "        .map(tokenizer_and_mask_wrapped, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        # Shuffle\n",
    "        .shuffle(100000)\n",
    "        # Form minibatches\n",
    "        # Use drop_remainder=True to force the batch size to be exactly TEST_BATCH_SIZE\n",
    "        # and make the shape **exactly** (TEST_BATCH_SIZE, SEQ_LENGTH)\n",
    "        #.batch(TEST_BATCH_SIZE, drop_remainder=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "meLGsVyoiW7u"
   },
   "outputs": [],
   "source": [
    "train_client_data_all_merged = train_client_data.create_tf_dataset_for_client(\n",
    "    train_client_data.client_ids[0]).filter(check_empty_snippet)\n",
    "\n",
    "if len(train_client_data.client_ids) > 1:\n",
    "    for i in range(1, len(train_client_data.client_ids)):\n",
    "        train_client_data_all_merged = train_client_data_all_merged.concatenate(\n",
    "            train_client_data.create_tf_dataset_for_client(train_client_data.client_ids[i]).filter(check_empty_snippet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66831,
     "status": "ok",
     "timestamp": 1605903831791,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "K-YFkR4CDhhC",
    "outputId": "90d2d1e6-d840-4e89-e734-821a70ea5a4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /content/drive/My Drive/Colab Notebooks/BERTerated/datasets/preprocessing_for_bert.py:76: Bernoulli.__init__ (from tensorflow.python.ops.distributions.bernoulli) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /content/drive/My Drive/Colab Notebooks/BERTerated/datasets/preprocessing_for_bert.py:76: Bernoulli.__init__ (from tensorflow.python.ops.distributions.bernoulli) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/distributions/bernoulli.py:97: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/distributions/bernoulli.py:97: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    }
   ],
   "source": [
    "train_client_data_all_merged = preprocess_for_train(train_client_data_all_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66823,
     "status": "ok",
     "timestamp": 1605903831796,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "YohjkEIZEJ__",
    "outputId": "9db6df46-7584-493d-cf1e-ff40e0dadee6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(1, 128), dtype=tf.int32, name=None),\n",
       " TensorSpec(shape=(1, 128), dtype=tf.int32, name=None),\n",
       " TensorSpec(shape=(1, 128), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_client_data_all_merged.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-X8beaHakOge"
   },
   "outputs": [],
   "source": [
    "test_client_data_all_merged = test_client_data.create_tf_dataset_for_client(\n",
    "    test_client_data.client_ids[0]).filter(check_empty_snippet)\n",
    "\n",
    "if len(test_client_data.client_ids) > 1:\n",
    "    for i in range(1, len(test_client_data.client_ids)):\n",
    "        test_client_data_all_merged = test_client_data_all_merged.concatenate(\n",
    "            test_client_data.create_tf_dataset_for_client(test_client_data.client_ids[i]).filter(check_empty_snippet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wj_jUe1MnVpx"
   },
   "outputs": [],
   "source": [
    "test_client_data_all_merged = preprocess_for_test(test_client_data_all_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLmZiK9iGAFN"
   },
   "outputs": [],
   "source": [
    "test_input = list(test_client_data_all_merged.take(1).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 76105,
     "status": "ok",
     "timestamp": 1605903841107,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "LtNWTVKKGFSe",
    "outputId": "eeb57044-85f5-449d-c747-e0bd3439aa32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([[  101,  1996, 13410,   103,   103,  2004,  1045,   103,  1010,\n",
      "         2003,  6783,  2000,  6713,  1010,  1999,  1996,  3033,  2073,\n",
      "         2002, 11113,  1012,  1031,   103,  4237,  1033,   103,  2204,\n",
      "         1010,  2026, 17766,   103,  2000,  3531,  2017,  2007,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]], dtype=int32), array([[ -100,  -100,  -100, 15367,  1010,  -100,  -100,  2963,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  4832,  -100,  -100,  3904,  -100,\n",
      "         -100,  -100,  -100,  1010,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100]], dtype=int32), array([[0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "      dtype=int32))]\n"
     ]
    }
   ],
   "source": [
    "print(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_o5NZpuzFQw7"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4279,
     "status": "ok",
     "timestamp": 1605903960770,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "kfTDbXNpnr1A",
    "outputId": "4d36f45f-cd75-4afa-963b-c4e0053b35f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMobileBertForPreTraining.\n",
      "\n",
      "All the layers of TFMobileBertForPreTraining were initialized from the model checkpoint at google/mobilebert-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMobileBertForPreTraining for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "mobilebert_model = transformers.TFMobileBertForPreTraining.from_pretrained(\n",
    "    'google/mobilebert-uncased', cache_dir='./transformers_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 83885,
     "status": "ok",
     "timestamp": 1605903848913,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "4xrKChglOi9J",
    "outputId": "e702472a-caa6-4575-e005-fa9ae5693168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileBertConfig {\n",
      "  \"_name_or_path\": \"google/mobilebert-uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_activation\": false,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"relu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"intra_bottleneck_size\": 128,\n",
      "  \"key_query_shared_bottleneck\": true,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"mobilebert\",\n",
      "  \"normalization_type\": \"no_norm\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_feedforward_networks\": 4,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"trigram_input\": true,\n",
      "  \"true_hidden_size\": 128,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bottleneck\": true,\n",
      "  \"use_bottleneck_attention\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(mobilebert_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDOF1EcWrOVJ"
   },
   "outputs": [],
   "source": [
    "# Due to the limitations with Keras subclasses, we can only use the main layer part from pretrained models\n",
    "# and add output heads by ourselves\n",
    "mobilebert_keras_converted = utils.convert_huggingface_mlm_to_keras(\n",
    "    huggingface_model=mobilebert_model,\n",
    "    max_seq_length=BERT_MAX_SEQ_LENGTH,\n",
    "    batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 568,
     "status": "ok",
     "timestamp": 1605903968528,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "uTbzF5dzxKeL",
    "outputId": "dfe73e1a-0b1a-4353-cfe1-6a8a5678687a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(1, 128)]                0         \n",
      "_________________________________________________________________\n",
      "mobilebert (TFMobileBertMain ((1, 128, 512), (1, 512)) 24581888  \n",
      "_________________________________________________________________\n",
      "standalone_tf_mobile_bert_ml (1, 128, 30522)           15921466  \n",
      "=================================================================\n",
      "Total params: 40,503,354\n",
      "Trainable params: 40,503,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mobilebert_keras_converted.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qM6T_Mp8FQxQ"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXwPdo-_Qrsk"
   },
   "source": [
    "### Training setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWGC89ZVWt1c"
   },
   "outputs": [],
   "source": [
    "mobilebert_keras_converted.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-6),\n",
    "    loss=utils.CustomCrossEntropyError(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 899
    },
    "executionInfo": {
     "elapsed": 79302,
     "status": "error",
     "timestamp": 1605904062114,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "oqCXFc7gFQxl",
    "outputId": "18a40343-bba2-4aad-c843-e2105e4929e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "TensorShape([1, 128])\n",
      "TensorShape([1, 128, 30522])\n",
      "TensorShape([1, None])\n",
      "TensorShape([1, None, 30522])\n",
      "      1/Unknown - 0s 516us/step - loss: 0.0790 - accuracy: 0.0000e+00TensorShape([1, 128])\n",
      "TensorShape([1, 128, 30522])\n",
      "TensorShape([1, None])\n",
      "TensorShape([1, None, 30522])\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-96078b37b1bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m mobilebert_keras_converted.fit(\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_client_data_all_merged\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCENTRALIZED_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument:  Incompatible shapes: [1,128] vs. [1,0]\n\t [[node CustomCrossEntropyError/weighted_loss/Mul (defined at <ipython-input-32-96078b37b1bb>:3) ]]\n\t [[gradient_tape/CustomCrossEntropyError/RaggedToTensor_1/strided_slice/_70]]\n  (1) Invalid argument:  Incompatible shapes: [1,128] vs. [1,0]\n\t [[node CustomCrossEntropyError/weighted_loss/Mul (defined at <ipython-input-32-96078b37b1bb>:3) ]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_165085]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node CustomCrossEntropyError/weighted_loss/Mul:\n CustomCrossEntropyError/SparseSoftmaxCrossEntropyWithLogits/Reshape_2 (defined at <ipython-input-26-aab455ff4e60>:16)\n\nInput Source operations connected to node CustomCrossEntropyError/weighted_loss/Mul:\n CustomCrossEntropyError/SparseSoftmaxCrossEntropyWithLogits/Reshape_2 (defined at <ipython-input-26-aab455ff4e60>:16)\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "mobilebert_keras_converted.fit(\n",
    "    train_client_data_all_merged,\n",
    "    epochs=CENTRALIZED_EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9MGEXE0FB3F"
   },
   "outputs": [],
   "source": [
    "mobilebert_keras_converted.evaluate(\n",
    "    test_client_data_all_merged,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mobilebert_mlm_shakespeare_centralized.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
