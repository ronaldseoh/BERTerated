{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6qruq9ZFQsN"
   },
   "source": [
    "# Further Pre-training MobileBERT MLM with Federated Averaging (Stackoverflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-AgOn_qCFQsk"
   },
   "outputs": [],
   "source": [
    "# Copyright 2020, The TensorFlow Federated Authors.\n",
    "# Copyright 2020, Ronald Seoh\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zUGmvEbFQts"
   },
   "source": [
    "### Google Colab settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_1nBG9qaFQt9"
   },
   "outputs": [],
   "source": [
    "# Use Google Colab\n",
    "use_colab = True\n",
    "\n",
    "# Is this notebook running on Colab?\n",
    "# If so, then google.colab package (github.com/googlecolab/colabtools)\n",
    "# should be available in this environment\n",
    "\n",
    "# Previous version used importlib, but we could do the same thing with\n",
    "# just attempting to import google.colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    colab_available = True\n",
    "except:\n",
    "    colab_available = False\n",
    "\n",
    "if use_colab and colab_available:\n",
    "    # Mount Google Drive root directory\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # cd to the appropriate working directory under my Google Drive\n",
    "    %cd '/content/drive/My Drive/Colab Notebooks/BERTerated'\n",
    "    \n",
    "    # List the directory contents\n",
    "    !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xN8078Eoh-au"
   },
   "source": [
    "### CUDA Multi GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "urL8mY9mh-av"
   },
   "outputs": [],
   "source": [
    "# Use this code snippet to use specific GPUs\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Erojcp_oTfDn"
   },
   "outputs": [],
   "source": [
    "# IPython reloading magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "D_PBfwhLUQ03"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F16eB2w-FQuw"
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_physical_devices_gpu = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Allow the growth of GPU memory consumption to take place incrementally\n",
    "if tf_physical_devices_gpu:\n",
    "    for gpu in tf_physical_devices_gpu:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "k_SnruV2FQu0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Hello, World!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import datetime\n",
    "import json\n",
    "import pathlib\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow_federated as tff\n",
    "import tensorflow_text as tf_text\n",
    "import tensorflow_addons as tfa\n",
    "import transformers\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import fedavg\n",
    "import fedavg_client\n",
    "import datasets\n",
    "import utils\n",
    "\n",
    "\n",
    "# Random seed settings\n",
    "random_seed = 692\n",
    "random.seed(random_seed) # Python\n",
    "np.random.seed(random_seed) # NumPy\n",
    "tf.random.set_seed(random_seed) # TensorFlow\n",
    "\n",
    "# Test if TFF is working\n",
    "tff.federated_computation(lambda: 'Hello, World!')()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FHYxELiQFQvI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.6.9 (default, Jul 17 2020, 12:50:27) \n",
      "[GCC 8.4.0]\n",
      "NumPy version: 1.18.4\n",
      "TensorFlow version: 2.3.1\n",
      "TensorFlow Federated version: 0.17.0\n",
      "Transformers version: 3.4.0\n"
     ]
    }
   ],
   "source": [
    "# Print version information\n",
    "print(\"Python version: \" + sys.version)\n",
    "print(\"NumPy version: \" + np.__version__)\n",
    "print(\"TensorFlow version: \" + tf.__version__)\n",
    "print(\"TensorFlow Federated version: \" + tff.__version__)\n",
    "print(\"Transformers version: \" + transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KGCWO7YprvKD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 28 06:56:44 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 1080    On   | 00000000:03:00.0 Off |                  N/A |\r\n",
      "|  0%   36C    P2    46W / 205W |    231MiB /  8118MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QSEmIf8Rh-az"
   },
   "outputs": [],
   "source": [
    "tf_logical_devices_cpu = tf.config.list_logical_devices('CPU')\n",
    "tf_logical_devices_gpu = tf.config.list_logical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYXMEboAFQvd"
   },
   "source": [
    "## Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "M071lqOQFQvi"
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_CONFIG = {}\n",
    "\n",
    "EXPERIMENT_CONFIG['HUGGINGFACE_MODEL_NAME'] = 'google/mobilebert-uncased'\n",
    "EXPERIMENT_CONFIG['HUGGINGFACE_CACHE_DIR'] = os.path.join('.', 'transformers_cache')\n",
    "\n",
    "EXPERIMENT_CONFIG['TOTAL_ROUNDS'] = 50 # Number of total training rounds\n",
    "EXPERIMENT_CONFIG['ROUNDS_PER_EVAL'] = 1 # How often to evaluate\n",
    "\n",
    "EXPERIMENT_CONFIG['TRAIN_CLIENTS_PER_ROUND'] = 10 # How many clients to sample per round.\n",
    "EXPERIMENT_CONFIG['TEST_CLIENTS_PER_ROUND'] = 10\n",
    "EXPERIMENT_CONFIG['CLIENT_EPOCHS_PER_ROUND'] = 3\n",
    "\n",
    "EXPERIMENT_CONFIG['BATCH_SIZE'] = 16 # Batch size used on the client.\n",
    "EXPERIMENT_CONFIG['TEST_BATCH_SIZE'] = 16 # Minibatch size of test data.\n",
    "\n",
    "# Maximum length of input token sequence for BERT.\n",
    "EXPERIMENT_CONFIG['BERT_MAX_SEQ_LENGTH'] = 128\n",
    "\n",
    "# Optimizer configuration\n",
    "EXPERIMENT_CONFIG['SERVER_LEARNING_RATE'] = 1.0 # Server learning rate.\n",
    "EXPERIMENT_CONFIG['CLIENT_LEARNING_RATE'] = 5e-5 # Client learning rate\n",
    "\n",
    "# Client dataset setting\n",
    "EXPERIMENT_CONFIG['TRAIN_NUM_CLIENT_LIMIT'] = 2000\n",
    "EXPERIMENT_CONFIG['TEST_NUM_CLIENT_LIMIT'] = 500\n",
    "\n",
    "# Path to save trained weights and logs\n",
    "EXPERIMENT_CONFIG['RESULTS_DIRECTORY'] = os.path.join(\n",
    "    '.', 'results',\n",
    "    'mobilebert_mlm_stackoverflow_fedavg',\n",
    "    datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    ")\n",
    "\n",
    "EXPERIMENT_CONFIG['RESULTS_LOG'] = os.path.join(EXPERIMENT_CONFIG['RESULTS_DIRECTORY'], \"logs\")\n",
    "EXPERIMENT_CONFIG['RESULTS_MODEL'] = os.path.join(EXPERIMENT_CONFIG['RESULTS_DIRECTORY'], \"model\")\n",
    "EXPERIMENT_CONFIG['RESULTS_CONFIG'] = os.path.join(EXPERIMENT_CONFIG['RESULTS_DIRECTORY'], \"config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_mg9h33jTfDu"
   },
   "outputs": [],
   "source": [
    "# Dump all the configuration into a json file\n",
    "pathlib.Path(EXPERIMENT_CONFIG['RESULTS_CONFIG']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(EXPERIMENT_CONFIG['RESULTS_CONFIG'], \"config.json\"), 'w') as config_file:\n",
    "    json.dump(EXPERIMENT_CONFIG, config_file, indent=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "uPVWWix6h-a0"
   },
   "outputs": [],
   "source": [
    "# TFF executor factory settings\n",
    "# Reference: https://www.tensorflow.org/federated/api_docs/python/tff/backends/native/set_local_execution_context\n",
    "tff.backends.native.set_local_execution_context(\n",
    "    num_clients=EXPERIMENT_CONFIG['TRAIN_CLIENTS_PER_ROUND'],\n",
    "    max_fanout=100,\n",
    "    clients_per_thread=2,\n",
    "    server_tf_device=tf_logical_devices_cpu[0],\n",
    "    client_tf_devices=tf_logical_devices_cpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-tHcHX0FQvz"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ViLRf52DTfDv"
   },
   "source": [
    "### Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cVkK1BV9FQv3"
   },
   "outputs": [],
   "source": [
    "train_client_data = tff.python.simulation.hdf5_client_data.HDF5ClientData(\n",
    "    os.path.join('.', 'tff_cache', 'datasets', 'stackoverflow_train.h5'))\n",
    "# held_out_client_data = tff.python.simulation.hdf5_client_data.HDF5ClientData(\n",
    "#     os.path.join('.', 'tff_cache', 'datasets', 'stackoverflow_held_out.h5'))\n",
    "test_client_data = tff.python.simulation.hdf5_client_data.HDF5ClientData(\n",
    "    os.path.join('.', 'tff_cache', 'datasets', 'stackoverflow_test.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNYlFQJ3Wt1O"
   },
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZXmJYQbeFQwG"
   },
   "outputs": [],
   "source": [
    "bert_tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    EXPERIMENT_CONFIG['HUGGINGFACE_MODEL_NAME'], cache_dir=EXPERIMENT_CONFIG['HUGGINGFACE_CACHE_DIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8c7S9s9qRlsi"
   },
   "outputs": [],
   "source": [
    "# Imitate transformers tokenizer with TF.Text Tokenizer\n",
    "tokenizer_tf_text, vocab_lookup_table, special_ids_mask_table = \\\n",
    "datasets.preprocessing_for_bert.convert_huggingface_tokenizer(bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sfMNvpmFQwW"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "oIOw1zG1RfnU"
   },
   "outputs": [],
   "source": [
    "def check_empty_snippet(x):\n",
    "    return tf.strings.length(x['tokens']) > 0\n",
    "\n",
    "def tokenizer_and_mask_wrapped(x):\n",
    "\n",
    "    masked, labels = datasets.preprocessing_for_bert.tokenize_and_mask(tf.reshape(x['tokens'], shape=[1]),\n",
    "                                                                       max_seq_length=EXPERIMENT_CONFIG['BERT_MAX_SEQ_LENGTH'],\n",
    "                                                                       bert_tokenizer_tf_text=tokenizer_tf_text,\n",
    "                                                                       vocab_lookup_table=vocab_lookup_table,\n",
    "                                                                       special_ids_mask_table=special_ids_mask_table,\n",
    "                                                                       cls_token_id=bert_tokenizer.cls_token_id,\n",
    "                                                                       sep_token_id=bert_tokenizer.sep_token_id,\n",
    "                                                                       pad_token_id=bert_tokenizer.pad_token_id,\n",
    "                                                                       mask_token_id=bert_tokenizer.mask_token_id)\n",
    "\n",
    "    return (masked, labels)\n",
    "\n",
    "def preprocess_for_train(train_dataset):\n",
    "    return (\n",
    "        train_dataset\n",
    "        # Tokenize each samples using MobileBERT tokenizer\n",
    "        .map(tokenizer_and_mask_wrapped, num_parallel_calls=tf.data.experimental.AUTOTUNE, deterministic=False)\n",
    "        # Shuffle\n",
    "        .shuffle(100000)\n",
    "        # Form minibatches\n",
    "        # Use drop_remainder=True to force the batch size to be exactly BATCH_SIZE\n",
    "        # and make the shape **exactly** (BATCH_SIZE, SEQ_LENGTH)\n",
    "        .batch(EXPERIMENT_CONFIG['BATCH_SIZE'])\n",
    "        # NOTE: THIS SHOULD BE COMMENTED OUT FOR CENTRALIZED TRAINING\n",
    "        #.repeat(count=EXPERIMENT_CONFIG['CENTRALIZED_EPOCHS'])\n",
    "    )\n",
    "    \n",
    "def preprocess_for_test(test_dataset):\n",
    "    return (\n",
    "        test_dataset\n",
    "        # Tokenize each samples using MobileBERT tokenizer\n",
    "        .map(tokenizer_and_mask_wrapped, num_parallel_calls=tf.data.experimental.AUTOTUNE, deterministic=False)\n",
    "        # Shuffle\n",
    "        .shuffle(100000)\n",
    "        # Form minibatches\n",
    "        # Use drop_remainder=True to force the batch size to be exactly TEST_BATCH_SIZE\n",
    "        # and make the shape **exactly** (TEST_BATCH_SIZE, SEQ_LENGTH)\n",
    "        .batch(EXPERIMENT_CONFIG['TEST_BATCH_SIZE'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjtIARunTfDw"
   },
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "kJHkMtw9oTZ4"
   },
   "outputs": [],
   "source": [
    "train_client_data = train_client_data.preprocess(preprocess_fn=lambda x: x.filter(check_empty_snippet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "zMIo2trUC7F7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/BERTerated/datasets/preprocessing_for_bert.py:76: Bernoulli.__init__ (from tensorflow.python.ops.distributions.bernoulli) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/BERTerated/datasets/preprocessing_for_bert.py:76: Bernoulli.__init__ (from tensorflow.python.ops.distributions.bernoulli) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/distributions/bernoulli.py:97: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/distributions/bernoulli.py:97: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    }
   ],
   "source": [
    "train_client_data = train_client_data.preprocess(preprocess_fn=preprocess_for_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "8QkftPB_TfDx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), TensorSpec(shape=(None, 128), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "print(train_client_data.element_type_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oQFfWtG1TfDw"
   },
   "outputs": [],
   "source": [
    "# Since the dataset is pretty large, we randomly select TRAIN_NUM_CLIENT_LIMIT number of clients.\n",
    "all_train_client_ids = train_client_data.client_ids\n",
    "\n",
    "random.shuffle(all_train_client_ids)\n",
    "\n",
    "if EXPERIMENT_CONFIG['TRAIN_NUM_CLIENT_LIMIT'] > 0:\n",
    "    selected_train_client_ids = all_train_client_ids[0:EXPERIMENT_CONFIG['TRAIN_NUM_CLIENT_LIMIT']]\n",
    "else:\n",
    "    selected_train_client_ids = all_train_client_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list of selected training clients\n",
    "with open(os.path.join(EXPERIMENT_CONFIG['RESULTS_CONFIG'], \"train_clients_list.json\"), 'w') as train_clients_file:\n",
    "    json.dump(selected_train_client_ids, train_clients_file, indent=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_client_states = {}\n",
    "\n",
    "# Initialize client states for all clients (selected for the entire simulation)\n",
    "for i, client_id in enumerate(selected_train_client_ids):\n",
    "    train_client_states[client_id] = fedavg_client.ClientState(client_serial=i, num_processed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioWWeZ6_TfD3"
   },
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "U_yi6VEzTfD3"
   },
   "outputs": [],
   "source": [
    "test_client_data = test_client_data.preprocess(preprocess_fn=lambda x: x.filter(check_empty_snippet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "vhUf1XLyh-a1"
   },
   "outputs": [],
   "source": [
    "test_client_data = test_client_data.preprocess(preprocess_fn=preprocess_for_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "HjHKEEk2TfD3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), TensorSpec(shape=(None, 128), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "print(test_client_data.element_type_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the dataset is pretty large, we randomly select TEST_NUM_CLIENT_LIMIT number of clients.\n",
    "all_test_client_ids = test_client_data.client_ids\n",
    "\n",
    "random.shuffle(all_test_client_ids)\n",
    "\n",
    "if EXPERIMENT_CONFIG['TEST_NUM_CLIENT_LIMIT'] > 0:\n",
    "    selected_test_client_ids = all_test_client_ids[0:EXPERIMENT_CONFIG['TEST_NUM_CLIENT_LIMIT']]\n",
    "else:\n",
    "    selected_test_client_ids = all_test_client_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list of selected training clients\n",
    "with open(os.path.join(EXPERIMENT_CONFIG['RESULTS_CONFIG'], \"test_clients_list.json\"), 'w') as test_clients_file:\n",
    "    json.dump(selected_test_client_ids, test_clients_file, indent=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_o5NZpuzFQw7"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "kfTDbXNpnr1A"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMobileBertForPreTraining.\n",
      "\n",
      "All the layers of TFMobileBertForPreTraining were initialized from the model checkpoint at google/mobilebert-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMobileBertForPreTraining for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_model = transformers.TFAutoModelForPreTraining.from_pretrained(\n",
    "    EXPERIMENT_CONFIG['HUGGINGFACE_MODEL_NAME'], cache_dir=EXPERIMENT_CONFIG['HUGGINGFACE_CACHE_DIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "jm1u4FUsTfD4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileBertConfig {\n",
      "  \"_name_or_path\": \"google/mobilebert-uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_activation\": false,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"relu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"intra_bottleneck_size\": 128,\n",
      "  \"key_query_shared_bottleneck\": true,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"mobilebert\",\n",
      "  \"normalization_type\": \"no_norm\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_feedforward_networks\": 4,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"trigram_input\": true,\n",
      "  \"true_hidden_size\": 128,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bottleneck\": true,\n",
      "  \"use_bottleneck_attention\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(bert_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "FDOF1EcWrOVJ"
   },
   "outputs": [],
   "source": [
    "# Due to the limitations with Keras subclasses, we can only use the main layer part from pretrained models\n",
    "# and add output heads by ourselves\n",
    "bert_keras_converted = utils.convert_huggingface_mlm_to_keras(\n",
    "    huggingface_model=bert_model,\n",
    "    max_seq_length=EXPERIMENT_CONFIG['BERT_MAX_SEQ_LENGTH'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "tblHhwjqEqfw"
   },
   "outputs": [],
   "source": [
    "# Use lists of NumPy arrays to backup pretained weights\n",
    "bert_pretrained_trainable_weights = []\n",
    "bert_pretrained_non_trainable_weights = []\n",
    "\n",
    "for w in bert_keras_converted.trainable_weights:\n",
    "    bert_pretrained_trainable_weights.append(w.numpy())\n",
    "\n",
    "for w in bert_keras_converted.non_trainable_weights:\n",
    "    bert_pretrained_non_trainable_weights.append(w.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "2kRecwJPFQw-"
   },
   "outputs": [],
   "source": [
    "def tff_model_fn():\n",
    "    \"\"\"Constructs a fully initialized model for use in federated averaging.\"\"\"\n",
    "\n",
    "    loss = utils.MaskedLMCrossEntropy()\n",
    "\n",
    "    model_wrapped = utils.KerasModelWrapper(\n",
    "        tf.keras.models.clone_model(bert_keras_converted),\n",
    "        train_client_data.element_type_structure, loss)\n",
    "\n",
    "    return model_wrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qM6T_Mp8FQxQ"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXwPdo-_Qrsk"
   },
   "source": [
    "### Training setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "bWzYrhYBTfD5"
   },
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.create_file_writer(EXPERIMENT_CONFIG['RESULTS_LOG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "vPNLTNLWQwDX"
   },
   "outputs": [],
   "source": [
    "def server_optimizer_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=EXPERIMENT_CONFIG['SERVER_LEARNING_RATE'])\n",
    "\n",
    "def client_optimizer_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=EXPERIMENT_CONFIG['CLIENT_LEARNING_RATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "qBzCiCkWFQxW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/operators/control_flow.py:565: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/operators/control_flow.py:565: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 16s, sys: 3.94 s, total: 2min 20s\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "iterative_process = fedavg.build_federated_averaging_process(\n",
    "    model_fn=tff_model_fn,\n",
    "    model_input_spec=train_client_data.element_type_structure,\n",
    "    initial_trainable_weights=bert_pretrained_trainable_weights,\n",
    "    initial_non_trainable_weights=bert_pretrained_non_trainable_weights,\n",
    "    server_optimizer_fn=server_optimizer_fn, \n",
    "    client_optimizer_fn=client_optimizer_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "yGAWSNTksOmF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 s, sys: 3.23 s, total: 23.2 s\n",
      "Wall time: 23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "server_state = iterative_process.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "DpdQjiHATfD5"
   },
   "outputs": [],
   "source": [
    "metric_eval = tfa.metrics.MeanMetricWrapper(fn=utils.calculate_masked_lm_cross_entropy, name='ce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "qWGC89ZVWt1c"
   },
   "outputs": [],
   "source": [
    "# The model for calculating validation loss only\n",
    "# (This happens outside FedAvg)\n",
    "model_final = utils.KerasModelWrapper(\n",
    "    tf.keras.models.clone_model(bert_keras_converted),\n",
    "    train_client_data.element_type_structure,\n",
    "    utils.MaskedLMCrossEntropy(),\n",
    "    tf_device_identifier=\"/GPU:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u923t4MTTfD5"
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "oqCXFc7gFQxl",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 start!\n",
      "Selected client serials: [1504 1797 1327 1890  350  211 1292 1645  997 1034]\n",
      "Anonymous client 1890 : updated the model with server message.\n",
      "Anonymous client 1890 : training start.\n",
      "Anonymous client 1890 : batch 1 , 16 examples processed\n",
      "Anonymous client 1890 : batch 2 , 32 examples processed\n",
      "Anonymous client 1890 : batch 3 , 48 examples processed\n",
      "Anonymous client 1890 : batch 4 , 64 examples processed\n",
      "Anonymous client 1890 : batch 5 , 80 examples processed\n",
      "Anonymous client 1890 : batch 6 , 96 examples processed\n",
      "Anonymous client 1890 : batch 7 , 112 examples processed\n",
      "Anonymous client 1890 : batch 8 , 128 examples processed\n",
      "Anonymous client 1890 : batch 9 , 134 examples processed\n",
      "Anonymous client 1890 : training finished. 134  examples processed, loss: 10.3256588\n",
      "Anonymous client 997 : updated the model with server message.\n",
      "Anonymous client 997 : training start.\n",
      "Anonymous client 997 : batch 1 , 16 examples processed\n",
      "Anonymous client 997 : batch 2 , 32 examples processed\n",
      "Anonymous client 997 : batch 3 , 48 examples processed\n",
      "Anonymous client 997 : batch 4 , 64 examples processed\n",
      "Anonymous client 997 : batch 5 , 80 examples processed\n",
      "Anonymous client 997 : batch 6 , 96 examples processed\n",
      "Anonymous client 997 : batch 7 , 102 examples processed\n",
      "Anonymous client 997 : training finished. 102  examples processed, loss: 10.3256264\n",
      "Anonymous client 1292 : updated the model with server message.\n",
      "Anonymous client 1292 : training start.\n",
      "Anonymous client 1327 : updated the model with server message.\n",
      "Anonymous client 1327 : training start.\n",
      "Anonymous client 1327 : batch 1 , 16 examples processed\n",
      "Anonymous client 1292 : batch 1 , 16 examples processed\n",
      "Anonymous client 1327 : batch 2 , 32 examples processed\n",
      "Anonymous client 1292 : batch 2 , 32 examples processed\n",
      "Anonymous client 1327 : batch 3 , 48 examples processed\n",
      "Anonymous client 1292 : batch 3 , 48 examples processed\n",
      "Anonymous client 1327 : batch 4 , 64 examples processed\n",
      "Anonymous client 1292 : batch 4 , 64 examples processed\n",
      "Anonymous client 1327 : batch 5 , 80 examples processed\n",
      "Anonymous client 1292 : batch 5 , 80 examples processed\n",
      "Anonymous client 1504 : updated the model with server message.\n",
      "Anonymous client 1504 : training start.\n",
      "Anonymous client 1504 : batch 1 , 16 examples processed\n",
      "Anonymous client 1034 : updated the model with server message.\n",
      "Anonymous client 1034 : training start.\n",
      "Anonymous client 1327 : batch 6 , 96 examples processed\n",
      "Anonymous client 1292 : batch 6 , 96 examples processed\n",
      "Anonymous client 1034 : batch 1 , 16 examples processed\n",
      "Anonymous client 1504 : batch 2 , 32 examples processed\n",
      "Anonymous client 1327 : batch 7 , 112 examples processed\n",
      "Anonymous client 1292 : batch 7 , 112 examples processed\n",
      "Anonymous client 1034 : batch 2 , 32 examples processed\n",
      "Anonymous client 1504 : batch 3 , 48 examples processed\n",
      "Anonymous client 1327 : batch 8 , 126 examples processed\n",
      "Anonymous client 1034 : batch 3 , 48 examples processed\n",
      "Anonymous client 1292 : batch 8 , 128 examples processed\n",
      "Anonymous client 1504 : batch 4 , 64 examples processed\n",
      "Anonymous client 1327 : training finished. 126  examples processed, loss: 10.3257742\n",
      "Anonymous client 1034 : batch 4 , 64 examples processed\n",
      "Anonymous client 1292 : batch 9 , 144 examples processed\n",
      "Anonymous client 1504 : batch 5 , 80 examples processed\n",
      "Anonymous client 1034 : batch 5 , 80 examples processed\n",
      "Anonymous client 1292 : batch 10 , 160 examples processed\n",
      "Anonymous client 1504 : batch 6 , 96 examples processed\n",
      "Anonymous client 1034 : batch 6 , 96 examples processed\n",
      "Anonymous client 1292 : batch 11 , 176 examples processed\n",
      "Anonymous client 1504 : batch 7 , 112 examples processed\n",
      "Anonymous client 1034 : batch 7 , 112 examples processed\n",
      "Anonymous client 1292 : batch 12 , 192 examples processed\n",
      "Anonymous client 1504 : batch 8 , 128 examples processed\n",
      "Anonymous client 1034 : batch 8 , 114 examples processed\n",
      "Anonymous client 1292 : batch 13 , 208 examples processed\n",
      "Anonymous client 1034 : training finished. 114  examples processed, loss: 10.3257799\n",
      "Anonymous client 1504 : batch 9 , 144 examples processed\n",
      "Anonymous client 1645 : updated the model with server message.\n",
      "Anonymous client 1645 : training start.\n",
      "Anonymous client 1645 : batch 1 , 16 examples processed\n",
      "Anonymous client 1292 : batch 14 , 224 examples processed\n",
      "Anonymous client 1504 : batch 10 , 160 examples processed\n",
      "Anonymous client 1645 : batch 2 , 32 examples processed\n",
      "Anonymous client 1292 : batch 15 , 240 examples processed\n",
      "Anonymous client 1504 : batch 11 , 176 examples processed\n",
      "Anonymous client 1645 : batch 3 , 48 examples processed\n",
      "Anonymous client 1292 : batch 16 , 256 examples processed\n",
      "Anonymous client 1504 : batch 12 , 192 examples processed\n",
      "Anonymous client 1645 : batch 4 , 64 examples processed\n",
      "Anonymous client 1292 : batch 17 , 272 examples processed\n",
      "Anonymous client 1504 : batch 13 , 204 examples processed\n",
      "Anonymous client 350 : updated the model with server message.\n",
      "Anonymous client 350 : training start.\n",
      "Anonymous client 1645 : batch 5 , 80 examples processed\n",
      "Anonymous client 350 : batch 1 , 16 examples processed\n",
      "Anonymous client 1292 : batch 18 , 288 examples processed\n",
      "Anonymous client 1504 : training finished. 204  examples processed, loss: 10.325326\n",
      "Anonymous client 1645 : batch 6 , 96 examples processed\n",
      "Anonymous client 1292 : batch 19 , 304 examples processed\n",
      "Anonymous client 350 : batch 2 , 32 examples processed\n",
      "Anonymous client 1645 : batch 7 , 112 examples processed\n",
      "Anonymous client 1292 : batch 20 , 320 examples processed\n",
      "Anonymous client 350 : batch 3 , 48 examples processed\n",
      "Anonymous client 1645 : batch 8 , 128 examples processed\n",
      "Anonymous client 1292 : batch 21 , 336 examples processed\n",
      "Anonymous client 350 : batch 4 , 64 examples processed\n",
      "Anonymous client 1292 : batch 22 , 352 examples processed\n",
      "Anonymous client 1645 : batch 9 , 144 examples processed\n",
      "Anonymous client 350 : batch 5 , 80 examples processed\n",
      "Anonymous client 211 : updated the model with server message.\n",
      "Anonymous client 211 : training start.\n",
      "Anonymous client 1645 : batch 10 , 152 examples processed\n",
      "Anonymous client 1292 : batch 23 , 368 examples processed\n",
      "Anonymous client 350 : batch 6 , 96 examples processed\n",
      "Anonymous client 211 : batch 1 , 16 examples processed\n",
      "Anonymous client 1645 : training finished. 152  examples processed, loss: 10.3256798\n",
      "Anonymous client 1292 : batch 24 , 384 examples processed\n",
      "Anonymous client 350 : batch 7 , 112 examples processed\n",
      "Anonymous client 211 : batch 2 , 32 examples processed\n",
      "Anonymous client 1292 : batch 25 , 400 examples processed\n",
      "Anonymous client 350 : batch 8 , 128 examples processed\n",
      "Anonymous client 211 : batch 3 , 48 examples processed\n",
      "Anonymous client 1292 : batch 26 , 416 examples processed\n",
      "Anonymous client 350 : batch 9 , 144 examples processed\n",
      "Anonymous client 211 : batch 4 , 64 examples processed\n",
      "Anonymous client 211 : batch 5 , 80 examples processed\n",
      "Anonymous client 1292 : batch 27 , 432 examples processed\n",
      "Anonymous client 350 : batch 10 , 160 examples processed\n",
      "Anonymous client 1292 : batch 28 , 448 examples processed\n",
      "Anonymous client 211 : batch 6 , 96 examples processed\n",
      "Anonymous client 350 : batch 11 , 176 examples processed\n",
      "Anonymous client 211 : batch 7 , 112 examples processed\n",
      "Anonymous client 1292 : batch 29 , 464 examples processed\n",
      "Anonymous client 350 : batch 12 , 185 examples processed\n",
      "Anonymous client 350 : training finished. 185  examples processed, loss: 10.3254957\n",
      "Anonymous client 1292 : batch 30 , 480 examples processed\n",
      "Anonymous client 211 : batch 8 , 128 examples processed\n",
      "Anonymous client 1292 : batch 31 , 496 examples processed\n",
      "Anonymous client 211 : batch 9 , 144 examples processed\n",
      "Anonymous client 1292 : batch 32 , 512 examples processed\n",
      "Anonymous client 211 : batch 10 , 160 examples processed\n",
      "Anonymous client 1292 : batch 33 , 528 examples processed\n",
      "Anonymous client 211 : batch 11 , 176 examples processed\n",
      "Anonymous client 1292 : batch 34 , 544 examples processed\n",
      "Anonymous client 211 : batch 12 , 192 examples processed\n",
      "Anonymous client 1292 : batch 35 , 560 examples processed\n",
      "Anonymous client 211 : batch 13 , 208 examples processed\n",
      "Anonymous client 1292 : batch 36 , 576 examples processed\n",
      "Anonymous client 211 : batch 14 , 224 examples processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anonymous client 1292 : batch 37 , 592 examples processed\n",
      "Anonymous client 211 : batch 15 , 240 examples processed\n",
      "Anonymous client 1292 : batch 38 , 608 examples processed\n",
      "Anonymous client 211 : batch 16 , 256 examples processed\n",
      "Anonymous client 1292 : batch 39 , 624 examples processed\n",
      "Anonymous client 211 : batch 17 , 272 examples processed\n",
      "Anonymous client 1292 : batch 40 , 640 examples processed\n",
      "Anonymous client 211 : batch 18 , 288 examples processed\n",
      "Anonymous client 1292 : batch 41 , 656 examples processed\n",
      "Anonymous client 211 : batch 19 , 304 examples processed\n",
      "Anonymous client 1292 : batch 42 , 672 examples processed\n",
      "Anonymous client 211 : batch 20 , 320 examples processed\n",
      "Anonymous client 1292 : batch 43 , 688 examples processed\n",
      "Anonymous client 211 : batch 21 , 336 examples processed\n",
      "Anonymous client 1292 : batch 44 , 704 examples processed\n",
      "Anonymous client 211 : batch 22 , 347 examples processed\n",
      "Anonymous client 1292 : batch 45 , 720 examples processed\n",
      "Anonymous client 211 : training finished. 347  examples processed, loss: 10.3242359\n",
      "Anonymous client 1292 : batch 46 , 736 examples processed\n",
      "Anonymous client 1292 : batch 47 , 752 examples processed\n",
      "Anonymous client 1292 : batch 48 , 768 examples processed\n",
      "Anonymous client 1292 : batch 49 , 784 examples processed\n",
      "Anonymous client 1292 : batch 50 , 800 examples processed\n",
      "Anonymous client 1292 : batch 51 , 816 examples processed\n",
      "Anonymous client 1292 : batch 52 , 832 examples processed\n",
      "Anonymous client 1292 : batch 53 , 848 examples processed\n",
      "Anonymous client 1292 : batch 54 , 864 examples processed\n",
      "Anonymous client 1292 : batch 55 , 880 examples processed\n",
      "Anonymous client 1292 : batch 56 , 896 examples processed\n",
      "Anonymous client 1292 : batch 57 , 912 examples processed\n",
      "Anonymous client 1292 : batch 58 , 928 examples processed\n",
      "Anonymous client 1292 : batch 59 , 944 examples processed\n",
      "Anonymous client 1292 : batch 60 , 960 examples processed\n",
      "Anonymous client 1292 : batch 61 , 976 examples processed\n",
      "Anonymous client 1292 : batch 62 , 992 examples processed\n",
      "Anonymous client 1292 : batch 63 , 1008 examples processed\n",
      "Anonymous client 1292 : batch 64 , 1024 examples processed\n",
      "Anonymous client 1292 : batch 65 , 1040 examples processed\n",
      "Anonymous client 1292 : batch 66 , 1056 examples processed\n",
      "Anonymous client 1292 : batch 67 , 1072 examples processed\n",
      "Anonymous client 1292 : batch 68 , 1088 examples processed\n",
      "Anonymous client 1292 : batch 69 , 1104 examples processed\n",
      "Anonymous client 1292 : batch 70 , 1120 examples processed\n",
      "Anonymous client 1292 : batch 71 , 1136 examples processed\n",
      "Anonymous client 1292 : batch 72 , 1152 examples processed\n",
      "Anonymous client 1292 : batch 73 , 1168 examples processed\n",
      "Anonymous client 1292 : batch 74 , 1184 examples processed\n",
      "Anonymous client 1292 : batch 75 , 1200 examples processed\n",
      "Anonymous client 1292 : batch 76 , 1216 examples processed\n",
      "Anonymous client 1292 : batch 77 , 1232 examples processed\n",
      "Anonymous client 1292 : batch 78 , 1248 examples processed\n",
      "Anonymous client 1292 : batch 79 , 1264 examples processed\n",
      "Anonymous client 1292 : batch 80 , 1280 examples processed\n",
      "Anonymous client 1292 : batch 81 , 1296 examples processed\n",
      "Anonymous client 1292 : training finished. 1296  examples processed, loss: 10.3210878\n",
      "Anonymous client 1797 : updated the model with server message.\n",
      "Anonymous client 1797 : training start.\n",
      "Anonymous client 1797 : batch 1 , 16 examples processed\n",
      "Anonymous client 1797 : batch 2 , 32 examples processed\n",
      "Anonymous client 1797 : batch 3 , 48 examples processed\n",
      "Anonymous client 1797 : batch 4 , 64 examples processed\n",
      "Anonymous client 1797 : batch 5 , 80 examples processed\n",
      "Anonymous client 1797 : batch 6 , 96 examples processed\n",
      "Anonymous client 1797 : batch 7 , 112 examples processed\n",
      "Anonymous client 1797 : batch 8 , 128 examples processed\n",
      "Anonymous client 1797 : batch 9 , 144 examples processed\n",
      "Anonymous client 1797 : batch 10 , 160 examples processed\n",
      "Anonymous client 1797 : batch 11 , 176 examples processed\n",
      "Anonymous client 1797 : batch 12 , 192 examples processed\n",
      "Anonymous client 1797 : batch 13 , 208 examples processed\n",
      "Anonymous client 1797 : batch 14 , 224 examples processed\n",
      "Anonymous client 1797 : batch 15 , 240 examples processed\n",
      "Anonymous client 1797 : batch 16 , 256 examples processed\n",
      "Anonymous client 1797 : batch 17 , 272 examples processed\n",
      "Anonymous client 1797 : batch 18 , 288 examples processed\n",
      "Anonymous client 1797 : batch 19 , 304 examples processed\n",
      "Anonymous client 1797 : batch 20 , 320 examples processed\n",
      "Anonymous client 1797 : batch 21 , 336 examples processed\n",
      "Anonymous client 1797 : batch 22 , 348 examples processed\n",
      "Anonymous client 1797 : training finished. 348  examples processed, loss: 10.3248568\n",
      "Round 1 training loss: 10.323409080505371\n",
      "Round 1 execution time: 792.8538987636566\n",
      "\n",
      "Updating client states.\n",
      "\n",
      "Recording client statistics:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with summary_writer.as_default():\n",
    "    for round_num in range(1, EXPERIMENT_CONFIG['TOTAL_ROUNDS'] + 1):        \n",
    "        # FedAvg\n",
    "        print(f'Round {round_num} start!')\n",
    "\n",
    "        # Training client selection\n",
    "        sampled_client_serials = np.random.choice(\n",
    "            len(selected_train_client_ids),\n",
    "            size=EXPERIMENT_CONFIG['TRAIN_CLIENTS_PER_ROUND'],\n",
    "            replace=False)\n",
    "\n",
    "        sampled_train_data = [\n",
    "            train_client_data.create_tf_dataset_for_client(selected_train_client_ids[client_serial])\n",
    "            for client_serial in sampled_client_serials\n",
    "        ]\n",
    "        \n",
    "        sampled_client_states = [\n",
    "            train_client_states[selected_train_client_ids[client_serial]]\n",
    "            for client_serial in sampled_client_serials\n",
    "        ]\n",
    "        \n",
    "        print(\"Selected client serials:\", sampled_client_serials)\n",
    "\n",
    "        current_round_start_time = time.time()\n",
    "        \n",
    "        server_state, new_client_states, train_loss = iterative_process.next(\n",
    "            server_state, sampled_client_states, sampled_train_data)\n",
    "        \n",
    "        current_round_end_time = time.time()\n",
    "        \n",
    "        currnt_round_running_time = current_round_end_time - current_round_start_time\n",
    "\n",
    "        print(f'Round {round_num} training loss: {train_loss}')\n",
    "        print(f'Round {round_num} execution time: {currnt_round_running_time}')\n",
    "        \n",
    "        # Record the current round's training loss to the log\n",
    "        tf.summary.scalar('train_loss', train_loss, step=round_num)\n",
    "        tf.summary.scalar('train_running_time', currnt_round_running_time, step=round_num)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # Update client states\n",
    "        print(\"Updating client states.\")\n",
    "\n",
    "        for state in new_client_states:\n",
    "            train_client_states[selected_train_client_ids[state.client_serial]] = state\n",
    "\n",
    "        print()\n",
    "        \n",
    "        print(\"Recording client statistics:\")\n",
    "        \n",
    "        for client_id in selected_train_client_ids:\n",
    "            state = train_client_states[client_id]\n",
    "            \n",
    "            tf.summary.scalar(\n",
    "                'client_' + str(int(state.client_serial)) + '_num_processed',\n",
    "                int(state.num_processed), step=round_num)\n",
    "\n",
    "        print()\n",
    "\n",
    "        # Evaluation\n",
    "        if round_num % EXPERIMENT_CONFIG['ROUNDS_PER_EVAL'] == 0:\n",
    "            model_final.from_weights(server_state.model_weights)\n",
    "\n",
    "            # Test dataset generation for this round\n",
    "            print(\"Calculating validation metric:\")\n",
    "            \n",
    "            # Select test clients\n",
    "            sampled_test_data = [\n",
    "                test_client_data.create_tf_dataset_for_client(test_client_id)\n",
    "                for test_client_id in selected_test_client_ids\n",
    "            ]\n",
    "\n",
    "            current_round_validation_start_time = time.time()\n",
    "            \n",
    "            current_round_validation_metric = utils.keras_evaluate(\n",
    "                model_final.keras_model, sampled_test_data, metric_eval, \"/GPU:0\")\n",
    "            \n",
    "            current_round_validation_end_time = time.time()\n",
    "            \n",
    "            current_round_validation_runnning_time = current_round_validation_end_time - current_round_validation_start_time\n",
    "\n",
    "            print(f'Round {round_num} validation metric: {current_round_validation_metric}')\n",
    "            print(f'Round {round_num} validation time: {current_round_validation_runnning_time}')\n",
    "            \n",
    "            # Write down train_metrics to the log\n",
    "            tf.summary.scalar('validation_metric', current_round_validation_metric, step=round_num)\n",
    "            tf.summary.scalar('validation_running_time', current_round_validation_runnning_time, step=round_num)\n",
    "            \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxbkOyonTfD7"
   },
   "source": [
    "### Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "NT7Xr0o0TfD7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./results/mobilebert_mlm_stackoverflow_fedavg/20201128-065644/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./results/mobilebert_mlm_stackoverflow_fedavg/20201128-065644/model/assets\n"
     ]
    }
   ],
   "source": [
    "model_final.keras_model.save(EXPERIMENT_CONFIG['RESULTS_MODEL'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mobilebert_mlm_shakespeare_fedavg.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "362px",
    "width": "629px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
